---
title: "[MP] Intorduction to Deep Learning"
excerpt: "딥러닝"
categories:
  - ECE
  - MP
tags:
  - 2-1

toc: true
toc_label: "Deep Learning"
toc_icon: star
---
CNN은 학부 저학년부터 해야 한다. 올해 처음으로 2학년 수업에 강조하고 있다. 알고리즘을 이해하고 설계하는 것이 굉장히 중요해서 강조하는 것이다.

## Introduction to Deep Learning

- CNN은 내적을 열심히 하는데 내적의 실체가 곱셈이라고 했을 때
- 사람의 뇌를 잘 이해하고 그것을 모방하여 만들었다. 선형적인 분류를 할 수 있다. XOR로 대표되는 non linear한 것들을 다룰 수 없다.
- G Hinton이 지금의 Deep Learning의 문을 연 사람이다. Perceptron을 여러 번 중첩하면 non linear한 것들을 다룰 수 있더라.
- 2012년 Deep Neural Network로 혁신적인 결과를 냈다. 이 시점부터 대회에서 딥러닝을 한 것들이 많아지더니 지금은 다 딥러닝이다.
- Perceptron은 1950년까지 거슬러 올라간다.

## Perceptron

- Cluster가 있는 것들 즉 linear한 것들은 다룰 수가 있지만 non linear한 것들은 다룰 수 없다.
- non linear한 것들을 어떻게 classification할 수 있을까?

## Multi-Layer Perceptron

- 하나 하나의 뉴런들이 Multi-Layer로 되어 있다.
- 제프리 힌턴

## Beyond MLP and the Challenges

- 첫 번째 뉴런에서 나온 결과를 다음 뉴런에 입력으로 넣는 구조를 만들어 Layer 층을 쌓으면 복잡한 문제를 풀 수 있다.
- Hidden Layer를 3층 쌓은 것이다.
- 곱해서 누적한 후 결과를 출력한다. 각각의 값을 더해서 1이 넘지 않도록. output layer로 나온 것은 확률적인 것이다.
- 고양이 사진이 pixel로 되어 있는데 각각을 계산하는 것이다.
- 분류할 수 있는 클래스의 종류가 4개면 output layer가 네 개다.
- MLP에서는 하나의 뉴런이 그 전 층의 모든 뉴런과 연결되는 반면, 딥러닝에서는 하나의 뉴런이 그 전 층에 모든 뉴런과 연결된 것이 아니라 일부와만 연결된다.
- 만약 올바른 결과가 나오지 않는다면 문제의 원인을 찾기 위해 wight에 대한 gredient를 찾는다.
- 학습의 실체는 각 layer의 weight 값을 구하는 것이다. 수학적 실체는 weight의 gredient를 미분하는 것이다.
- 모든 탐색 공간에서 최적값은 아니지만 주어진 것에서 최적값을 찾는 식으로 주어진 공간에서 최적값을 찾아나간다
- 정답이 주어져야 한다.

## DNN

- GPU가 나온 것도 Deep Learning 때문이다. CPU로는 계산 능력이 만족스럽지 않아서다. GPU의 killer application이 Deep Learning이었다.
- GPU, BIG DATA, Algorithm 삼박자가 맞아서 Deep Learning이 나올 수 있었다.
- Machine Learning에서 귀납적인 방법을 취하는 것이 Deep Learning이다. 개념이 없이 엄청나게 많은 유형의 문제를 풀어서 실력을 올리는 것과 같다.
- feature을 제공하지 않아도 이미지만 많이 제공하면 되더라. Supervised Learning
- 자연어 처리, 이미지 등은 그 값이 나오도록 label 값을 수정해나가면 된다.

## Perceptron(2)

- 곱셈과 덧셈을 할 수 있는 ALU로 벡터의 내적을 한다. 텐서 코어가 4 by 4 연산을 할 수 있다.
- 내부적인 데이터 패스는 비슷하지만
- 범용의 명령을 수행할 수 있는 프로세서가 아니라 딥러닝과 관련된 연산만을 할 수 있는 것이다. 행렬 등의 연산에 특화되어 있다.
- RISC-V를 집적해서 만든 CNN 가속기를 구현할 수 있다.
- 내적의 실체는 어떤 값을 곱한 다음에 더하는 것이다. Convolatinoal neural network
- 알고리즘적으로 단순해 보이는 것이 어떻게 좋은 성능을 낼 수 있을까?
- 사람이 feature을 정하고 .... 2010년대까지의 접근
- Deep Learning은 feature를 정하지 않는다. Feature extraction 자체를 학습한다. 각각에 레이어에 있는 weight들이 학습된다.
- weight 값이 바뀌면서 확정되면 올바른 feature가 정해진다.
- 그렇게 얻어진 고차원 feature를 가지고 분류를 한다.(분류기)
- 앞에서 feature를 학습하는 데 쓰이고, 뒤에서 multi way perceptron만큼 대상을 class로 분류한다.
- cnn을 설계할 때 layer를 정해야 한다.

## Face Recognition using DNN

- 고차원 필터(weight들의 묶음)를 이용해서 다음의 뉴런 값을 결정한다.
- 다음 뉴런에 대해서도 동일한 필터의 값을 곱한다.
- 이전에 있는 모든 레이어의 뉴런 값과 관련이 있다.
- weight 값이 동일하다.
- 레이어를 새롭게 만들고 있다. 한 점을 새롭게 계산해야 한다. 픽셀 하나.
- 픽셀 값을 정할 때 이전 레이어를 거쳐야 다음 레이어로 갈 수 있다.
- 하나의 필터 값으로부터 레이어의 뉴런 값이 결정된다.

## CNN

- multilayer(MLP)에 반해 CNN의 특징은 Partially-connected, Common weights
- 공통된 필터를 사용한다.
- 필터를 사용하는 CNN
- MLP와 CNN(congruential nearul network) 중 MLP가 메모리를 훨씬 많이 쓴다. 계산량이 훨신 많다.
- 필터의 개수가 수십 ~ 수백, 1024개까지 있는 것도 있다.
- 레이어를 거치면서 필터의 개수가 많아진다.
- 어떤 이미지를 줬을 때 내적을 해서 얻은 뉴런 값들이 feature를 추출하여 방향성 갖는 edge를 뽑는다.
- 코나 눈이나 edge로 이루어져있기 때문에...
- RGB 세 채널이 있다.
- feature의 실체는 실수값으로 표현된 벡터다.
- ChatGPT의 실체는 word embeding을 한 것이다. 단어를 정량적으로 표현한다. 서울, 학교, 대구 등을 어떤 값으로 표현하고 학습을 시킨 것이다. 서울과 대구는 비슷 쪽에 있고 학교는 다른 쪽에 있을 것이다. 서울과 뉴옥이 각각 나라에서 차지하는 위상이 비슷하다고 한다면 계산이 가능하다. 어떤 것을 찾았을 때 뉴옥이 나올 수 있다. word track이라는 알고리즘으로 만들어졌다.
- 어떤 것을 뽑고 feature(vector)로 만들어서 classification을 하는 것이다. feature는 몇 차원 벡터를 의미한다.
- 자연어 처리는 앞에서 벡터를 만들고 단어와 단어의 관계를 이해한 후 처리한다. 순차적이다. 반면에 이미지는 순차적이지 않다.
- mlp의 첫 입력값은 벡터다.
- averaging : 내용을 줄이는 것이다.
- 중류에 따라 50개의 레이어에서 150개의 레이어가 많이 사용된다.
- 1000개의 레이어를 하면 어떨까? -> 더 좋아지지 않더라. 150개의 레이어를 넘어가는 경우가 많지 않다.
- 자연어 처리에서는 Transformer를 사용한다.
- 여러 layer를 쌓았을 때 인식성이 좋아진다.
- 왜 transformer가 더 좋을까?
- 라마라는 것이 opensource가 되었다. 학습시키는 데 쓰이는 자원이 엄청나기 때문에 많이 정보가 감춰졌다. game changer
- google이 검색의 왕 타이틀을 20년 동안 유지했지만 bing을 통해 검색해보면 더 낫다는 것을 알 수 있다.
- 이름만 open ai다. ms의 투자를 받았다. 구글이 바드를 만들었지만 후발주자가 되었다. 학부에서 배우는 것을 통해 어디까지 있는지 확인하고 재밌어서 보이는 정보를 스스로 찾아라.
- 동일한 input feature map
- 9개의 product를 하나로 누적하는 것...
- cpu는 내적을 하나씩 하지만 gpu는 동시에 진행된다.
- 병렬 처리...
- C 값은 점점 증가한다.
- Filter가 n개 있다고 하자.
- 2번째 filter는 output의 2번째 channel에 해당된다. filter가 output channel로 들어간다.

## CNN Computation

- innput feature map:
- Output channel들의 plain은 독립적이다.
- train data set이 있어야 한다.
- CNN의 연산량이 깔끔하게 된다.
- convolution algorithm으로 가속을 할 수 있다.
- 한 레이어의 입력이 완성되어야 다음 레이어가 학습될 수 있다.
- 등록된 사람인지 회부인인지 구분할 수 있디.
- 새로운 필터마다 동일한 입력에 대해 계산하는 것이 맞다.
- Q. M 자원의 channel을 가지는 filter의 연산은 병렬적으로 동시에 수행될 수 있는가? - > ㅇ
- 인접한 두 뉴런들의 집합을 레이어라고 하는 것이 맞다.
- CNN은 DNN에 속하는 것이다. DNN 안에 MLP도 있고 CNN도 있다.
- mlp만을 이용하는 게 아니라 CNN도 이용한다.
- Q. 7layer에서 반복문의 N M F E는 각각 어떤 의미인지 궁금하다.
- 하나의 congrution layer에서 수행되는 연산의 실체다. 이 연산이 하나의 레이어에서 이루어진다.
- CNN 구조를 묻는 문제가 출제될 것이다.
