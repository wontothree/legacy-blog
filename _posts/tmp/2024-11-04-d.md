---
title: "[Tmp] Tmp"
categories:
  - tmp
---
$p_{\theta}(x)$가 probability density function이려면 다음의 조건을 만족해야 한다.

1. $p_{\theta}(x) \geq 0$
2. $\int p_{\theta}(x) dx = 1$

# Energy-Based Model

$$
p_{\theta}(x) = \dfrac{\exp (-\varepsilon_{\theta}(x))}{Z_{\theta}}, \;\;\; Z_{\theta} = \int \exp (-\varepsilon_{\theta}(x)) d\mathbf{x}
$$

Maximum likelihood training

$$
\nabla_{\theta} \log p_{\theta}(x) = -\nabla_{\theta} \varepsilon_{\theta}(x) - \nabla_{\theta} \log Z_{\theta}
$$

EBM의 단점은 normalizing constant를 계산하기가 어렵다는 것이다.

$$
\begin{align*}
  \nabla_{\theta} \log Z_{\theta}
  &= \nabla_{\theta} \log \left( \int \exp (- \varepsilon_{\theta}(x)) d\mathbf{x} \right) \\
  &= \dfrac{\nabla_{\theta} \left( \int \exp (- \varepsilon_{\theta}(x)) \right)}{\int \exp (- \varepsilon_{\theta}(x)) d\mathbf{x}} \\
  &= \dfrac{\int \nabla_{\theta} \exp (- \varepsilon_{\theta}(x))}{\int \exp (- \varepsilon_{\theta}(x)) d\mathbf{x}} \\
  &= \dfrac{\int \exp (-\varepsilon_{\theta}(x)) (-\nabla_{\theta} \varepsilon_{\theta}(x))}{\int \exp (- \varepsilon_{\theta}(x)) d\mathbf{x}} \\
  &= \int\dfrac{\exp (-\varepsilon_{\theta}(x)) (-\nabla_{\theta} \varepsilon_{\theta}(x))}{\int \exp (- \varepsilon_{\theta}(x)) d\mathbf{x}} \\
  &= \int p_{\theta}(x) (-\nabla_{\theta} \varepsilon_{\theta}(x))\\
  &= \mathbb{E}_{x \sim p_{\theta}} [ -\nabla_{\theta} \varepsilon_{\theta}(x)]
\end{align*}
$$

To train Energy-Based Model, we need to sample from it.

# Score Matching

Maximum likelihood training의 목적은 모델 분포와 데이터의 분포의 KLD를 $\theta$에 대해 최소화하는 것이다. 하지만 그것이 어렵기 때문에 score matching은 모델 분포의 스코어와 데이터 분포의 스코어에 대한 fisher divergence를 최소화한다. score를 사용한다면 normalizing constant를 무시할 수 있기 때문이다.

$$
\mathbf{s}_{\theta} (x) = \nabla_{x} \log p_{\theta}(x) = - \nabla_x \varepsilon_{\theta}(x)
$$

Cost function

$$
\begin{align*}
  \mathbb{E}_{p_{data}(\mathbf{x})} \left[ \dfrac{1}{2} \vert\vert \nabla_{\mathbf{x}} \log p_{\text{data}}(\mathbf{x}) - \nabla_{\mathbf{x}} \log p_{\theta}(\mathbf{x}) \vert\vert^2 \right] \\
  = \mathbb{E}_{p_{data}(\mathbf{x})} \left[ \vert\vert \mathbf{s}_{\theta}(\mathbf{x}) \vert\vert^2 + \text{tr} (\mathbf{J_x}\mathbf{s}_{\theta}(\mathbf{x})) \right] + C
\end{align*}
$$

장점

- 계산할 수 없는 $\nabla_{\mathbf{x}} \log p_{\theta}(\mathbf{x})$를 cost function에서 대체할 수 있는 방법을 제시한다.

Problem

- Jacobian trace of the score network is expensive to compute.
- 고차원에 대해 적용하기 어렵다. (scalibility)

# Denoising Score Matching

데이터 x에 Gaussian noise를 추가해 계산이 불가능한 $\nabla_x \log p_{\text{data}}(x)$을 계산이 가능한 식 $\nabla_{\tilde{x}} \log p_{\sigma}(\tilde{x})$으로 변형한다.

노이즈를 뿌린 데이터의 score matching은 쉽게 할 수 있다.

- $q(\tilde{\mathbf{x}} \vert \mathbf{x})$: noise distribution
- $q(\tilde{\mathbf{x}}) = \int q(\tilde{\mathbf{x}} \vert \mathbf{x}) p_{\text{data}}(\mathbf{x}) d\mathbf{x}$

For Gaussian Perturbation kernel,

$$
\nabla_{\mathbf{x}} \log q(\tilde{\mathbf{x}} \vert \mathbf{x}) = \nabla_{\mathbf{x}} \log \mathcal{N} ( \tilde{\mathbf{x}} \vert \mathbf{x}, \sigma^2\mathbf{I}) = \dfrac{\tilde{\mathbf{x}} - \mathbf{x}}{\sigma^2}
$$

$$
\begin{align*}
  \mathbb{E}_{q(\tilde{\mathbf{x}})} \left[ \dfrac{1}{2} \vert\vert \nabla_{\mathbf{x}} \log p_{\theta}(\tilde{\mathbf{x}}) - \nabla_{\mathbf{x}} \log q(\tilde{\mathbf{x}}) \vert\vert^2_2 \right]
  &=  \mathbb{E}_{q(\mathbf{x}, \tilde{\mathbf{x}})} \left[ \dfrac{1}{2} \vert\vert \nabla_{\mathbf{x}} \log p_{\theta}(\tilde{\mathbf{x}}) - \nabla_{\mathbf{x}} \log q(\tilde{\mathbf{x}} \vert \mathbf{x}) \vert\vert^2_2 \right] + C \\
  &=  \dfrac{1}{2} \mathbb{E}_{q(\mathbf{x}, \tilde{\mathbf{x}})} \left[ \left\vert\left\vert \mathbf{s}_{\theta}(\tilde{\mathbf{x}}) + \dfrac{\mathbf{x} - \tilde{\mathbf{x}}}{\sigma^2} \right\vert\right\vert_2^2 \right] + C \\
\end{align*}
$$

High dimentional data에서도 적용가능하다.

However we cannot learn the score of the true distribution

# Sampling with Langevin MCMC

$$
\tilde{\mathbf{x}}_t = \tilde{\mathbf{x}}_{t-1} + \dfrac{\epsilon}{2}\nabla_{\mathbf{x}} \log p(\tilde{\mathbf{x}}_{t-1}) + \sqrt{\epsilon}\mathbf{z}_{t}
$$

2가지 단점이 있다.

1. Inaccurate score estimation in the low-density regions
2. Relative Weight를 고려하지 못한다.

Mixture of two disjoint를 고려하자.

$$
p_{\text{data}}(\mathbf{x}) = (1 - \pi) p_1(\mathbf{x}) + \pi p_2(\mathbf{x})
$$

다음과 같이 weight에 대한 정보가 없어진다.

$$
\nabla_{\mathbf{x}} p_{\text{data}}(\mathbf{x}) = \nabla_{\mathbf{x}} \log p_1(\mathbf{x})
$$

# Generative Modeling by Estimating Gradients of the Data Distribution

# Score-Based Generative Modeling Through Stochastic Differential Equations

Contribution

1. A generalized framework that unifies the Diffusion Probabilistic Models and Score-Based Models through SDE
2. Reveals connection of DPMs and continuous-time normalizing flows