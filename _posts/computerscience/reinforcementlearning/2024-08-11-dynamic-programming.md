---
title: "[RL] Monte Carlo Method"
categories: 
  - reinforcementlearning
---
동적 프로그래밍으로 최적 가치 함수와 최적 정책을 찾을 수 있었다. DP를 이용하려면 환경 모델 (상태 전이 확률과 보상 함수)을 알고 있어야 한다. 하지만 세상에는 환경 모델을 알 수 없는 문제가 많으며, 알 수 있더라도 DP 방식으로는 계산량이 너무 많아서 사실상 풀 수 없을 때가 많다.

강화학습은 이처럼 환경 모델을 알 수 없는 상황에서 더 나은 정책을 찾는 문제를 주로 다룬다. 이런 상황에서 문제를 풀려면 에이전트가 실제로 행동하여 얻은 경험을 토대로 학습해야 한다.

몬테카를로법 : 데이터를 반복적으로 샘플링하여 그 결과를 토대로 추정하는 방법

강화학습에서는 몬테카를로법을 통해 경험으로부터 가치 함수를 추정할 수 있다. 여기서 경험은 환경과 에이전트가 실제로 상호작용하여 얻은 데이터(일련의 상태, 행동, 보상)이다.

이번 장의 목표는 에이전트가 얻은 경험을 바탕으로 가치 함수를 추정하는 것이다. 이 목표가 달성되면 이어서 최적 정책을 찾을 수 있다.

## 1. 몬테카를로법 기초

지금까지는 환경 모델이 알려진 문제를 다뤘다. 그리드 월드 문제에서는 에이전트의 행동에 따른 다음 상태와 보상이 명확했다. 이처럼 환경 모델이 알려진 문제에서는 에이전트 측에서 '상태, 행동, 보상'의 전이를 시뮬레이션 할 수 있다.

하지만 현실에는 황경 모델을 알 수 없는 문제가 많으며, 상태 전이 확률을 이론적으로 알 수 있더라도 계산량이 너무 많은 경우가 허다하다.

얼마나 많은지 실감할 수 있도록 이번 절에서는 우선 주사위를 예로 들어 간단한 작업을 해보겠다.

### 1.1 주사위 눈의 합

주사위 두 개를 굴리는 문제를 가정하겠다. 각 눈이 나올 확률은 정확하게 1/6이라고 가정하자.

확률 분포를 얻을 수 있다.

|주사위 눈의 합|2|3|4|5|6|7|8|9|10|11|12|
|---|---|---|---|---|---|---|---|---|---|---|---|
|확률|$\dfrac{1}{36}$|$\dfrac{2}{36}$|$\dfrac{3}{36}$|$\dfrac{4}{36}$|$\dfrac{5}{36}$|$\dfrac{6}{36}$|$\dfrac{5}{36}$|$\dfrac{4}{36}$|$\dfrac{3}{36}$|$\dfrac{2}{36}$|$\dfrac{1}{36}$|

이와 같이 확률 분포를 알면 기댓값을 얻을 수 있다.

```py
ps = {2: 1/36, 3: 2/36, 4: 3/36, 5: 4/36, 6: 5/36, 7: 6/36, 8: 5/36, 9: 4/36, 10: 3/36, 11: 2/36, 12: 1/36}

V = 0
for x, p in ps.items():
    V += x * p
print(V) # 6.99999999999999
```

### 1.2 분포 모델과 샘플 모델

우리는 주사위를 굴리는 시도를 확률 분포로 모델링하였다.

모델링을 표현하는 방법

- 분포 모델 : 확률 분포로 표현된 모델
  - 조건 : 확률 분포를 정확하게 안다.
- 샘플 모델 : '표본을 추출(샘플링)할 수만 있으면 충분하다'라는 모델. 예를 들어 주사위를 실제로 굴려서(샘플링) 나온 눈의 합을 관찰하는 방법
  - 조건 : 샘플링할 수 있다.

### 1.3 몬테카를로법 구현

## 2. 몬테카를로법으로 정책 평가하기

### 2.1 가치 함수를 몬테카를로법으로 구하기

### 2.2모든 상태의 가치 함수 구하기

### 2.3 몬테카를로법 계산 효율 계산

## 3. 몬테카를로법 구현

### 3.1 step() 메서드

### 3.2 에이전트 클래스 구현

### 3.3 몬테카를로법 실행

## 4. 몬테카를로법으로 저액 제어하기

### 4.1 평가와 개선

### 4.2 몬테카를로법으로 정책 제어 구현

### 4.3 $\epsilon$-탐욕 정책으로 변경 (첫 번째 개선 사항)

### 4.4 고정값 $\alpha$ 방식으로 수행 (두 번째 개선 사항)

### 4.5 몬테카를로법으로 정책 반복법 구현 (개선 버전)

## 5. 오프-정책과 중요도 샘플링

### 5.1 온-정책과 오프-정책

### 5.2 중요도 샘플링

### 5.3 분산을 작게 하기
