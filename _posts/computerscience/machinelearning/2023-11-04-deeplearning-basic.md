---
title: "[Machine Learning] 딥러닝 기초"
excerpt: "Ilseok Oh - Machine Learning : Ch04"
categories:
  - machinelearning
---
깊은 신경망 : 다층 퍼셉트론에 은닉층을 여러 개 추가한 것

딥러닝 : 깊은 신경망을 학습시키는 알고리즘

## 1. 딥러닝의 등장

### 딥러닝의 기술 혁신 요인

- 1980년대 깊은 신경망에 대한 아이디어가 제시되었지만, 은닉층을 추가하여 MLP를 깊게 만들면 제대로 학습되지 않았다.
- 1990년대 부분적으로 딥러닝에 대한 긍정적인 연구 결과가 발표되기 시작하여 2000년대에 딥러닝이 꽃 피웠다.

|년도|문제/해결|설명|
|---|---|---|
|1980|그레디언트 소멸 문제|출력층에서 시작하여 입력층 방향으로 진행하면서 그레디언트를 계산하고 가중치를 갱신하는 역전파 알고리즘이 수행될 때, 여러 층을 거치면서 그레디언트 값이 작아져 입력층에 가까워지면 변화가 거의 없어졌다.|
||과잉적합 문제|늘어난 매개변수에 비해 훈련 집합이 작아서 과잉적합에 빠질 위험이 높아졌다.|
||과다한 계산 시간|병렬 처리를 하귀 위해 값비싼 슈퍼컴퓨터가 필요했다.|
|2000|컨볼루션 신경망|작은 크기의 컨볼루션 마스크를 사용하는 부분연결 구조와 가중치 공유 기법이 개발되었다.|
||값싼 GPU|대학 실험실에서도 병렬 처리를 할 수 있게 되었고, 학습 시간이 10~100배 단축되어 성능 실험을 다채롭게 할 수 있게 되었다.|
||큰 학습 데이터|인터넷을 통해 많은 학습 데이터를 수집할 수 있게 되었다.|
||성능이 좋은 활성함수|계산은 단순하지만 성능이 더 좋은 활성함수가 개발되었다.|
||학습에 효과적인 다양한 규제 기법|가중치 축소 기법, 드롭아웃 기법, 조기 멈춤, 데어터 확대, 앙상블|
||층별 예비학습|

### 특징 학습이 부각

|패러다임|수작업 특징|특징 학습|
|:---:|:---:|:---:|
|설명|사람이 시행착오를 거쳐 고안한 특징 추출 알고리즘으로 특징 벡터를 추출하여 신경망에 입력하는 접근 방식|기계학습에서 은닉칭이 계층적인 특징을 추출하는 방식|
|기계학습의 적용 범위|분류, 회귀|분류, 회귀, 생성 모델, 영상 분할 모델|

## 2. 깊은 다층 퍼셉트론

- MLP : 한 두 개의 은닉층을 가진 구조
- 깊은 MLP : 세 개 이상의 은닉층을 가진 구조

||퍼셉트론|MLP|DMLP|
|---|---|------|---|
|활성함수|계단함수|시그모이드 함수|ReLU 함수|
|목적함수|평균 제곱 오차|평균 제곱 오차|교차 엔트로피, 로그우도|

## 3. 컨볼로션 신경망

||DMLP|CNN|
|---|---|---|
|연결|완전 연결 구조|부분 연결 구조(희소 연결 구조)|
|모델의 복잡도|높다|낮다|
|특징|과잉적합 우려|좋은 특징 추출|
|두 층 사이 가중치의 수($n_{l-1}, n_l$)|$n_{l-1}*n_l$|$3*n_l$|
|입력 데이터 형식|벡터(1차원 배열)|행렬(2차원 배열), 텐서(3차원 배열)|
|입력 데이터 크기|항상 같은 크기|가변 크기|

CNN은 한 요소와 이웃 요소의 관련성이 깊은 격자 구조를 가진 데이터에 적합

- Activation map : 각 뉴런이 입력 데이터의 특정 부분에 얼마나 반응하는지 나타내는 지도 또는 배열. 컨볼루션 연산을 통해 얻어진다.

### 컨볼루션 연산

- 마스크 : 커널 또는 필터 또는 윈도
- 컨볼루션 : 해당하는 요소끼리 곱하고 그 결과를 모두 더하는 선형 연산
- 특징맵 : 컨볼루션의 결과

1차원 컨볼루션, 2차원 컨볼루션, 3차원 컨볼루션

컨볼루션 연산을 거치면 가장자리에서 총 h - 1개의 노드가 줄어든다. 깊은 신경망에서는 컨볼루션층이 여러 번 반복되므로 줄어드는 양이 많아져 문제가 된다.

덧데기, 가장자리에 인접한 노드값을 복사하는 방식

### 가중치 공유와 다중 특징 맵 추출

- 가중치 공유 또는 묶인 가중치 : 모든 노드가 같은 커널을 사용하여 가중치를 공유하는 기법

각각 8개의 노드를 가진 두 층 사이의 가중치의 개수

MLP : 8 * 8 = 64

CNN : 3

-> 모델의 복잡도가 크게 낮아진다.

커널이 어떤 값을 가지느냐에 따라 추출하느ㅜㄴ 특징이 달라진다.

### 컨볼루션 연산에 다른 CNN 연산

### 큰 보폭에 의한 다운샘플링

### 텐서에 적용

### Stanford

Q. image : 32 x 32 x 3 (가로 x 세로 x rgb), filter : 5 x 5 x 3. 만약 image의 모든 값이 1이고, 필터1의 한 값이 3이고 나머지는 0, 필터2의 한 값이 2이고 나머지 0, 필터3의 한 값이 -1이고 나머지 0이라면, 28 x 28 activation map의 모든 값은? -> 4

Q. why 28 x 28? 또한 stride에 따라 달라진다

Output size = (N - F) / stride + 1

컨볼루션 연산

- padding(덧데기) : 가장자리에서 영상의 크기가 줄어드는 효과 방지

병렬 분산 구조

- Stride(보폭) :

Pooling 

Stride를 크게 하면 downsampling이 가능하다.

pooling을 많이 하면 크기가 계속 작아진다.

DMLP : 특징 벡터의 크기가 달라지면 내부 구조를 다 바꿔야 하지만 CNN은 가변 크기를 다룰 수 있다.

## 4. 컨볼루션 신경망 사례 연구

||2011|2012|2013|2014|2015|
|---|---|---|---|---|---|
|우승 architecture|Xerox|AlexNet||GoogleNet(1등), VGGNet(2등)|ResNet|
|Team||토론토 대학|Clarifi|Google, 옥스퍼트|마이크로소프트|
|5순위 오류율|25.8%|15.3%||||
|특징|수작업|첫 CNN 참여||인셉션 모듈 / 3X3 커널(작은), 깊은 층|지름길 연결|

## 5. 생성 모델

|분별 모델|생성 모델|
|---|---|
|MLP, DMLP, CNN|GAN, VAE, RNN, RBM|
|지도 학습|비지도 학습|
|확률 분포x|확률 분포o|

### GAN

생성기(G)와 분별기(D)를 서로 대립시켜 학습한다. 생성기의 목적은 D가 진위 구별을 못하게 하는 것이고, 분별기는 훈련 집합의 진짜 샘플과 G가 만든 가짜 샘플을 구별한다.

DCGAN

## 6. 딥러닝은 왜 강력한가?

refernece : 오일석 - 기계학습