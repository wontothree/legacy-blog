---
title: "[Machine Learning] 딥러닝 최적화"
excerpt: "Ilseok Oh - Machine Learning : Ch05"
categories:
  - machinelearning
---
순수 최적화와 달리 기계학습은 학습이 끝난 후 현장에서 발생하는 새로운 샘플을 높은 성능으로 처리할 수 있어야 한다.

순수 최적화는 목적함수 자체가 궁극 목표를 직접 반영하는데, 기계 학습에서는 목적함수가 궁금 목표를 간접적으로 반영한다.

## 1. 목적 함수 : 교차 엔트로피와 로그우도

### 평균제곱 오차를 다시 생각하기

오류가 더 큰 상황에 그레디언트 값이 더 작아지는 현상이 발생한다.

### 교차 엔트로피 목적 함수

$$
H(P, Q) = -\sum_z P(X)\log_2Q(X)
$$

교차 엔트로피를 사용하면 오류가 클수록 더 큰 벌점을 부과한다.

### Softmax 활성함수와 로그우도 목적함수

Softmax 활성함수

$$
o_j = \dfrac{e^{s_j}}{\sum_{i=1, c}e^{s_j}}
$$

로지스틱 시그모이드는 입력 전보다 출력 후 차이가 줄어든다. 반면 softmax 활성함수는 최댓값을 더욱 활성화하고 작은 값을 억제한다.

Softmax의 출력값을 모두 더하면 1이 된다. 이에 따라 softmax의 출력을 확률분포를 취급할 수 있다.

Softmax는 최댓값이 아닌 값을 억제하여 0에 가깝게 만든다는 의도를 포함하며, 로그우도 목적함수는 학습샘플이 알려주는 부류에 해당하는 출력 노드값만 보겠다는 의도를 포함한다. Softmax 활성화 함수와 로그우도 목적함수는 잘 어울린다.

## 2. 성능 향상을 위한 요령

Neural Networks : Trcks of the Trade 라는 책에서는 성능 향상을 꾀하는 트릭과 휴리스틱을 다룬다.

벤지오의 논문에서 주목할 만한 구절이 있다.

"이 논문이 제시한 정제된 기법들은 자신의 문제에 적용한 다음 변형하여 새로운 기법을 만드는 길잡이 역할 정도로 받아들여야지 만고불변의 법칙으로 여겨서는 안 된다."

데이터가 달라지고 도메인이 달라지면 작동을 하지 않을 수 있다. 모든 데이터에 적용될 수는 없다.

### 데이터 전처리

건강에 관련된 데이터 (키, 몸무게, 협압)

키는 m 단위이고 몸무게는 kg 차이라 하면 첫 번째와 두 번째 특징은 100배의 규모 차이가 난다. 가중치 갱신이 100배 느려진다.

각 항목별로 데이터를 정규화하면 도움이 되더라.

평균을 빼면 부호 문제가 해결되고, 표준편차로 나누면 규모 문제가 해결된다.

### 가중치 초기화

대칭적 가중치 문제

난수로 가중치 초기화하면 되더라.

가우시안 분포를 가지고 난수를 생성할 수도 있고, 균일 분포를 기준으로 난수를 생성할 수도 있다.

대부분의 논문에서 둘 사이의 성능 차이가 없다는 것을 보이고 있다.

AlexNet : Breakthrough : 평균 0, 평균편차 0.001인 가우시안에서 난수 생성

ResNet : 평균 0, 표준편차 $\sqrt{\dfrac{2}{n_{in}}}$인 가우시안에서 난수 생성

### 모멘텀

대부분 적용한다.

샘플 하나하나에 그레디언트 디센트를 적용하면 잡음이 섞인다.

스무딩한다.

오버슈팅 현상을 누그러뜨림

네스테로프

학습 속도가 크게 향상 되더라.

### 적응적 학습률

학습률이 너무 크면 오버슈팅에 다른 진자 현상이 일어나고 너무 작으면 수렴 속도가 느려진다.

상수로 하지 않고 적응적으로 하면 도움이 되더라.

- AdaGrad

방금 전과 오래 전 정보를 동일하게 반영한다.

- RMSProp

오래 전 정보보다 방금 전 정보를 더 반영한다.

- Adam

RMSProp에 모멘텀을 같이 고려한다.

### 활성함수

시대별

1950 : 계산

1980 tanh

2000 ReLU

### 배치 정규화

학습을 느리게 하는 공변량 시프트 현상이 발생한다.

각 층의 입장에서 입력 데이터의 분포가 수시로 달라진다.

정규화를 적용하자.

훈련 집한 천제에 적용할 것이냐 미니 배치에 적용할 것이냐

실험 결과 미니 배치에 적용하는 것이 좋더라.

최적화를 마친 후 추가적인 후처리 작업이 필요하다.

계수들을 저장해두고 예측할 때 사용한다.

주의) CNN에서는 노드 단위가 아니라 특징 맵 단위로 적용해야 한다.

배치 정규화의 긍정적 효과

- 가중치 초기화에 덜 민감함
- 학습률을 크게 하여 수렴 속도 향상
- 시그모이드를 사용하여도 깊은 신경망 학습이 가능하더라.

## 3. 규제의 필요성과 원리

### 과잉적합에 빠지는 이유와 과잉적합을 피하는 전략

훈련집합에 대한 오류율은 모델의 용량이 클수록 0에 가까워지다가 최적 용량을 지나면 테스트집합에 대한 성능이 악화되어 일반화 능력이 점점 떨어진다.

설계자는 실용적인 문제를 풀어야 할 대 자신이 가진 데이터보다 작은 용량의 신경망을 설계하는 경우는 거의 없다. 반대로 훨씬 큰 용량으로 설계하는 일이 다반사다. 따라서 기계 학습에서는 과소적합보다 과잉적합을 어떻게 피할지에 관심을 집중한다.

가중치가 100만 개인데 샘플이 10만 개라면, 10개의 방정식으로 100만 개의 매개변수를 추정하는 문제를 풀어야 한다. 이러한 상황은 과잉적합의 주요 원인이다. 학습 알고리즘은 매개변숫값을 조정하는 일을 반복하다가 결국 주어진 데이터를 단순히 암기하는 상태가 될 수 있다.

데이터에 맞는 신경망을 설계하는 일은 불가능하다.

현대 기계 학습은 충분히 큰 용량의 신경망 구조를 설계한 다음, 학습 과정에서 여러 가지 규제 기법을 적용하는 전략을 구사한다.

### 규제의 정의

규제 기법은 기계 학습 이전부터 수학과 통계학에서 연구하였다.

- 분량 문제 : 모델 용량에 비해 데이터가 부족한 경우
- 사전 지식 : 데이터의 원천에 내재한 정보

가중치 감쇠

$$
J_{regularized}(\theta) = J(\theta) + \lambda R(\theta)
$$

- 딥러닝에서 규제 : 일반화 오류를 줄이려는 의도를 가지고 학습 알고리즘을 수정하는 방법 모두

## 4. 규제 기법

조기 멈춤 기법, 가중치 공유 기법, 데이터 확대 기법, 드롭아웃 기법, 앙상블 기법, 매니폴드 공간에서 매핑을 이용하는 기법

|규제 기법의 구분|예시|
|---|---|
|명시적 규제|가중치 감쇠, 드롭아웃|
|암시적 규제|조기 멈춤, 데이터 증대, 잡음 추가, 앙상블|

### 가중치 벌칙

### 조기 멈춤

### 데이터 확대

### 드롭아웃

입력층과 은닉층의 노드 중 일정 비율을 임의로 선택하여 제거하는 작업

- 앙상블 기법 : 여러 개의 예측기를 결합하는 접근 방식

### 앙상블 기법

- 앙상블 : 서로 다른 여러 개의 모델이 예측한 결과를 결합하여 일반화 오류를 줄이는 기법
- 배깅(bootstrap aggregating)

## 5. 하이퍼 매개변수 최적화

|매개변수의 종류|설명|
|---|---|
|모델 내부 매개변수|가중치 행렬의 내부 매개변수(에지 가중치)|
|하이퍼 매개변수|은닉층의 개수, CNN 마이크의 크기, 보폭, 활성함수와 관련된 매개변수, 학습률, 모멘텀과 관련된 매개변수|

학습률(하이퍼 파라미터 매개변수)이 너무 크면 오버슈팅 현상이 일어나 수렴하지 못할 위험이 있고, 너무 작게 하면 너무 느리게 수렴하는 문제가 있다.

하이퍼 파라미터를 선택하는 가장 중요한 방법은 표준 문헌이 제시하는 기본값을 참조하는 것이지만, 주어진 문제와 데이터에 맞는 값을 찾아내는 하이퍼 매개변수 최적화 문제를 풀어야 한다.

### 격자 탐색과 임의 탐색

|탐색 방식|설명|
|---|---|
|수동 탐색|사람이 일일이 조합을 만들어 목록에 넣은 다음 목록에서 하나씩 꺼내 평가하는 방식|
|격자 탐색||
|임의 탐색||

Adam은 3가지 하이퍼 매개변수 - 학습률, 모멘텀 계수, 가중 이동 평균 계수를 사용한다.

차원의 저주 측면을 고려하면 격자 탐색보다 임의 탐색이 유리하다.

## 6. 2차 미분을 이용한 최적화

## Reference

refernece : 오일석 - 기계학습

오일석 5장(5.2절)
