---
title: "[Machine Learning] 다층 퍼셉트론"
excerpt: "Ilseok Oh - Machine Learning : Ch03"
categories:
  - machinelearning
---
신경망은 기계 학습 역사에서 가장 오래되었으며, 형태가 가장 다양한 모델이다. 퍼셉트론은 가장 단순한 신경망 모델이다.

## 1. 신경망 기초

|신경망의 종류|설명|예시|
|---|---|---|
|전방 신경망|모든 계산이 오른쪽에서 왼쪽으로 진행된다.|퍼셉트론, 다층 퍼셉트론, 깊은 다층 퍼셉트론, 컨볼루션 신경망|
|순환 신경망|오른쪽에서 왼쪽으로 진행하는 피드백 계산도 포함한다.|순환 신경망(RNN), LSTM|
|얕은 신경망|은닉층이 1 ~ 2개 정도인 신경망|
|깊은 신경망|더 많은 은닉층을 가진 신경망|
|결정론 신경망|
|스토캐스틱 신경망|

## 2. 퍼셉트론

주어진 데이터가 선형 분리될 수 있다면 미분을 활용한 학습 알고리즘은 반드시 100% 정확률로 수렴한다는 정리가 증명되었다.

현대 신경망은 퍼셉트론을 병렬 구조와 순차 구조로 결합한 형태이다.

입력층과 출력층이 있는데, 입력층은 연산을 하지 않으므로 층의 개수를 셀 때 제외되어 퍼셉트론은 1개의 층이 있다고 말한다.

입력층에 있는 입력 노드 하나는 특징 벡터의 특징 하나에 해당한다.

입력 노드의 개수 = 특징벡터의 원소의 개수 + 1(바이어스)

## 3. 다층 퍼셉트론

|1969|1974|1986|
|---|---|---|
|민스키|웨어보스|Parallel Distributed Processing: Explorations in the Microstructure of Cognition|
|XOR 문제에서 드러나는 퍼셉트론의 한계|오류 역전파 알고리즘 제안|학습 알고리즘 '오류 역전파' 탄생|

### 특징 공간의 변환

XOR 문제

- 선형 분리 불가능
- 직선 모델을 사용하면 정확률 최대 75%

퍼셉트론 2개, 즉 결정 직선 2개를 이용하면 특징 공간을 3개의 영역, 즉 3개의 부분공간으로 나누어 XOR 문제에서 100%의 정확률을 얻을 수 있다.

## 4. 오류 역전파 알고리즘

### 목적함수의 정의

- 배치 모드 : 모든 샘플의 그레디언트를 계산한 후 한꺼번에 가중치를 갱신하는 방식

목적함수로 다층 퍼셉트론에서는 주로 평균 제곱 오차를 사용하고, 딥러닝은 주로 교차 엔트로피 또는 로그우도를 사용한다.

평균 제곱 오차 - 온라인 모드

$$
e = \dfrac{1}{2}||\mathbf{y} - \mathbf{o}||_2^2
$$

평균 제곱 오차 - 배치 모드

$$
e = \dfrac{1}{2n}||\mathbf{y} - \mathbf{o}||_2^2
$$

### 오류 역전파의 유도

- 오류 역전파 : 오류를 전파할 때 쓸 그레디언트 값을 구하는 방법

## 5. 미니배치 스토캐스틱 경사하강법

|경사 하강법의 종류|한 번에 처리하는 샘플의 개수(t)|
|---|---|
|스토캐스틱 경사 하강법|1|
|배치 경사 하강법|n|
|미니배치 경사 하강법|수십 ~ 수백|

한 샘플로 계산한 그레디언트는 잡음을 많이 포함하게 돼 최저점을 찾아가는 경로가 매개변수 공간에서 갈팡질팡하는 경향을 보인다.

배치와 스토캐스틱 방식에서는 세대마다 훈련집합의 모든 샘플이 빠지지 않고 충실히 참여한다. 하지만 미니배치 방식에서는 미니배치를 무작위로 뽑기 때문에 학습이 완료될 때까지 한 번도 학습에 참여하지 않은 샘플이 있을 수 았다. 하지만 미니배치가 모든 샘플의 대표성을 띠므로 이러한 현상이 미니배치 방식의 성능에 해를 끼치지 않는다고 실험적으로 입증되었다.

훈련집합을 미니배치 단위로 나누고 미니배치를 순서대로 처리하여 훈련집합을 모두 사용하도록 구현할 수도 있다. 이때 미니배치로 나누기 전에 훈련집합을 충분히 섞어야 한다.

미니배치는 배치 방식과 스토캐스틱 방식보다 무작위성이 높다. 이는 규제 효과를 가져다주어 미니배치 방식의 일반화 능력을 향상시킨다.

## 6. 다층 퍼셉트론의 인식

## 7. 다층 퍼셉트론의 특성

### 오류 역전파 알고리즘의 빠른 속도

학습 알고리즘의 계산 시간의 시간 복잡도

$$
\theta((dp + pc)nq)
$$

- d : 입력 노드의 개수
- p : 은닉 노드의 개수
- c : 출력 노드의 개수
- n : 샘플의 수
- q : 세대 수

### 모든 함수를 정확하게 근사할 수 있는 능력

"은닉 노드가 충분히 많다면, 활성함수로 무엇을 사용하든 표준 다층 퍼셉트론은 어떤 함수라도 원하는 정확도만큼 근사화할 수 있다.

### 성능 향상을 위한 휴리스틱의 중요성

신경망 연구자들은 성능을 높일 수 있는 갖가지 휴리스틱을 개발하고 공유하는 연구 문화를 누려왔다.

- 아키텍처 : 은닉층과 은닉 노드의 개수를 정해야 한다. 은닉층과 은닉 노드를 늘리면 신경망의 용량은 커지고, 추정할 매개변수가 많아지고 학습 과정에서 과잉적합할 가능성이 커진다.
- 초깃값 : 난수를 생성하여 설정하는데, 값의 범위와 분포가 중요하다.
- 학습률 : 처음부터 끝까지 같은 학습률을 사용하는 방식과 처음에는 큰 값으로 시작하고 점점 줄이는 적응적 방식이 있다.
- 활성함수 : 초창기 다층 퍼셉트론은 주로 로지스틱 시그모이드나 $\tanh$ 함수를 사용했자만, 깊은 신경망에서는 은닉층의 개수를 늘림에 따라 그레디언트 소멸과 같은 문제를 해결하기 위해 ReLU 함수를 사용한다.

refernece : 오일석 - 기계학습
