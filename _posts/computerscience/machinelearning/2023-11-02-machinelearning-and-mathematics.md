---
title: "[Machine Learning] 기계학습과 수학"
excerpt: "Ilseok Oh - Machine Learning : Ch02"
categories:
  - machinelearning
---
## 1. 선형대수

4가지의 특징을 수학적으로 어떻게 표현할 수 있을까? 모든 샘플을 하나의 수식으로 표현할 수 있을까? 기계학습 알고리즘은 데이터에 여러 가지 연산을 적용하는데, 연산 과정을 수학적으로 어떻게 표현할 수 있을까? 선형대수는 이러한 질문에 답을 준다.

설계 행렬 : 기계 학습에서 훈련집합을 담은 행렬

유사도를 이용하면 비슷한 샘플을 찾아 같은 군집으로 모으거나 가장 유사한 샘플을 찾아 그 샘플이 속한 부류로 분류할 수 있다.

## 2. 확률과 통계

### 정보이론

확률이 낮은 사건일수록 더 많은 정보를 전달한다.

자기 정보 : 정보량

밑이 2인 로그함수를 사용하는 경우 자기 정보의 단위는 비트이다. 확률이 1/2일 때 1비트의 정보량을 가진다. 반면 밑이 e인 자연로그를 사용하는 경우 단위는 니츠이다.

$$
h(e_i) = - \log_2P(e_i)
$$

자기 정보가 특정 사건의 정보량을 측정하는 반면, 엔트로피는 확률분포의 무질서도를 측정한다.

$$
H(x) = - \sum_{i=1, k}P(e_i)\log_2P(e_i)
$$

모든 사건이 동일한 확률을 가질 때 엔트로피가 최대이다.

서로 다른 두 확률분포 P와 Q 사이의 교차 엔트로피를 정의한다. 이때 두 확률분포는 같은 확률변수에 대해 정의되어 있어야 한다.

$$
H(P, Q) = - \sum_x P(X)\log_2Q(x)
$$

KL 다이버전스 : 두 확률분포가 얼마나 다른지 측정한다.

$$
KL(P || Q) = \sum_x P(X)\log_2 \dfrac{P(X)}{Q(X)}
$$

P와 Q의 교차 앤트로피 H(P, Q) = H(P) + $H(P, Q) = \sum_x P(X)\log_2Q(x)$

## 3. 최적화

기계 학습은 최적화 과정이다. 주어진 데이터를 가지고 최적화 문제를 어떻게 공식화할 것인가, 최적화 공식을 어떻게 풀어 최적의 해, 즉 목적함수를 최소로 하는 점을 찾을 것인가는 기계 학습의 핵심 주제이다.

미분은 극점을 찾아가는 데 필요한 방향 정보와 극점 도달 여부를 판정하는 정보를 제공하기 때문에 최적화 문제에서는 미분을 이용한다.

특징 공간에서의 확률분포를 정확하게 추론할 수 있다면 분류 문제를 완벽하게 풀었다고 할 수 있지만, 훈련 집합은 아주 높은 차원에 정의된 매우 희소한 데이터로 특징 공간의 확률분포를 구하는 것은 불가능하다. 따라서 기계 학습은 적절한 모델을 선택하고 목적함수를 정의하며 모델의 매개변수 공간을 탐색하여 목적함수가 최저가 되는 최적점을 찾아내는 전략을 사용한다. '특징 공간'에서 해야 하는 일을 모델의 매개변수 공간에서 하는 일로 대치'한 셈이다.

모델의 매개변수 공간은 특징 공간보다 수 배 ~ 수만 배 넓다.

기계 학습의 목적은 전역 최적해를 찾는 것이다. 실제로는 지역 최적해를 찾고 만족하는 경우도 많다.

낱낱 탐색 : 특징 공간의 각 차원을 구간으로 나누어 각각의 목적함숫값을 계산하고 그중 최저점을 찾는 방식

|최적화 문제 해결|설명|단점|
|---|---|---|
|낱낱 탐색|특징 공간의 각 차원을 구간으로 나누어 각각의 목적함숫값을 계산하고 그중 최저점을 찾는 방식|차원이 높아지면 계산량이 기하급수적으로 늘어나서 사용 불가능하다.|
|무작위 탐색|무작위로 선택된 점들을 탐색하는 방법. 시간을 설정하고 그 시간이 다하면 끝내는 방법 등 멈춤 조건이 있을 수 있다.||
|경사 하강법|

$-f'(x)$의 방향으로 가면 최저점을 찾을 수 있다.

편미분을 이용하여 다차원 공간에서 최저점을 찾을 수 있다.

야코비안과 헤시안은 여러 최적화 알고리즘에서 핵심 역할을 한다.

어떤 점 x에서서의 함숫값 $f(x)$와 미분 $f'(x)$가 주어지고 이웃한 점에서의 함숫값을 추정해야 하는 상황에서 테일러 급수를 이용할 수 있다.
